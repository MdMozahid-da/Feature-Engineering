{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "1gQhPIzbOBh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        " - A parameter is a numerical value that defines certain characteristics of a model or function. For instance, in a linear regression model, parameters include the coefficients of the variables and the intercept. Parameters are adjusted during training to minimize error and improve model predictions."
      ],
      "metadata": {
        "id": "98oix3vMOEas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "  \n",
        " - What does negative correlation mean? Correlation measures the statistical relationship between two variables. A negative correlation indicates that as one variable increases, the other decreases. For example, in many cases, hours of exercise and weight might be negatively correlated."
      ],
      "metadata": {
        "id": "JGVS694qOJzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning.What are the main components in Machine Learning?  \n",
        " - Machine Learning is the field of study that allows computers to learn from data and make predictions or decisions without being explicitly programmed. Its main components include:\n",
        "\n",
        " * Data: Input for training.\n",
        "\n",
        " * Model: A representation (e.g., linear regression).\n",
        "\n",
        " * Algorithms: Methods for training the model.\n",
        "\n",
        " * Loss Function: Measures prediction error.\n",
        "\n",
        " * Optimization: Updates parameters to reduce loss."
      ],
      "metadata": {
        "id": "Sl54ZWlGOOl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        " - The loss value quantifies the error between the model's predictions and the actual values. Lower loss indicates better predictions. For example, Mean Squared Error (MSE) evaluates the average squared difference between predicted and actual values."
      ],
      "metadata": {
        "id": "RBLarlEmOhW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        " - Continuous Variables: These take numeric values within a range (e.g., height, weight).\n",
        "\n",
        "  Categorical Variables: These represent discrete categories or labels (e.g., gender: Male or Female)."
      ],
      "metadata": {
        "id": "XuOvVKC-Ou-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning?\n",
        " - What are the common techniques? Categorical variables are encoded into numerical forms using techniques such as:\n",
        "\n",
        "    One-Hot Encoding: Converts categories into binary columns.\n",
        "\n",
        "    Label Encoding: Assigns numerical labels to categories."
      ],
      "metadata": {
        "id": "0HDX7aKWO3Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        " - Training a dataset involves using data to fit a model by finding patterns and relationships. Testing involves evaluating the model on unseen data to check its accuracy and generalizability."
      ],
      "metadata": {
        "id": "Bd6_MPX8O_jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        " - sklearn.preprocessing provides functions for data preprocessing tasks in Python, such as normalization, scaling, and encoding. For example, StandardScaler standardizes numerical data to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "LrXewFtqPEvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        " - A Test set is a subset of data used to evaluate a trained model. It helps determine how well the model performs on unseen data, ensuring accuracy and avoiding overfitting."
      ],
      "metadata": {
        "id": "w4Yi_59ePOBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        " - Data is split using sklearn's train_test_split function. For example:\n",
        " as shown in the python code\n",
        "\n",
        "     The Machine Learning approach includes:\n",
        "\n",
        "     * Understanding the problem.\n",
        "\n",
        "      * Preprocessing data.\n",
        "\n",
        "      * Choosing a model.\n",
        "\n",
        "     * Training and testing.\n",
        "\n",
        "     * Evaluating results."
      ],
      "metadata": {
        "id": "R5DhRW7QPTQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "'''"
      ],
      "metadata": {
        "id": "uH8V8VplP8p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        " - Exploratory Data Analysis (EDA) is a critical step before fitting a model to the data because it helps uncover patterns, anomalies, and relationships in the dataset. Here's why EDA is essential:\n",
        "\n",
        "     Data Cleaning: EDA helps identify missing values, outliers, or inconsistencies in the dataset. Cleaning these issues ensures the model isnâ€™t negatively impacted by noise or errors.\n",
        "\n",
        "     Understanding the Data: It provides insights into the dataset's structure, distributions, and variable types, helping you select appropriate algorithms and preprocessing techniques.\n",
        "\n",
        "     Feature Selection: EDA highlights which features are relevant for prediction and which ones may be redundant or irrelevant, improving model performance.\n",
        "\n",
        "     Detecting Relationships: It allows you to identify correlations or dependencies between variables, which can inform feature engineering or model selection.\n",
        "\n",
        "     Avoiding Bias: By exploring the data, you can spot biases or imbalances (e.g., class imbalances) that could skew the model's predictions, enabling you to address them beforehand.\n",
        "\n",
        "     Performing EDA ensures that the data is ready for modeling and that the model will yield meaningful and reliable results."
      ],
      "metadata": {
        "id": "eCIk94etQGHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. 2. What is correlation?\n",
        " - Correlation is a statistical measure that indicates the strength and direction of a relationship between two variables. It ranges from -1 to 1:\n",
        "\n",
        "     * +1: Perfect positive correlation (variables increase together).\n",
        "\n",
        "     * 0: No correlation (variables are independent).\n",
        "\n",
        "     * -1: Perfect negative correlation (one variable increases while the other decreases). For example, there is a positive correlation between the amount of time spent studying and exam scores."
      ],
      "metadata": {
        "id": "Op0N0ERdQSMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        " - Negative correlation refers to an inverse relationship between two variables, where an increase in one variable results in a decrease in the other. It is represented by correlation values between -1 and 0. For example, an increase in daily exercise time often leads to a decrease in body weight, indicating a negative correlation."
      ],
      "metadata": {
        "id": "GsVE5eLiUFeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        " - In Python, correlation between variables can be computed using the corr() method in pandas or the pearsonr() function from the scipy library. Here's an example using pandas:\n",
        "\n",
        " This will output the correlation matrix for the variables in the DataFrame."
      ],
      "metadata": {
        "id": "JFdyzamNUV1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {'X': [1, 2, 3, 4, 5], 'Y': [5, 4, 3, 2, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation\n",
        "correlation = df.corr()\n",
        "print(correlation)\n",
        "'''"
      ],
      "metadata": {
        "id": "MVwUnE50UdIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain the difference between correlation and causation with an example.\n",
        " -  Causation means that one variable directly causes a change in another.  \n",
        "     It implies a cause-and-effect relationship, unlike correlation, which only indicates a statistical relationship.\n",
        "\n",
        "      Example of Correlation: Ice cream sales and drowning incidents are correlated because both increase in summer.\n",
        "\n",
        "      Example of Causation: Turning on a light switch causes the light to turn on. Correlation does not imply causation, as correlations can occur due to coincidence or third variables."
      ],
      "metadata": {
        "id": "Krni7C5fUg2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        " - An optimizer is an algorithm used in Machine Learning to minimize the loss function by adjusting model parameters like weights and biases. Common optimizers include:\n",
        "\n",
        "     Gradient Descent: Updates parameters in the direction of the steepest descent of the loss function.\n",
        "\n",
        "     Adam (Adaptive Moment Estimation): Combines the advantages of RMSProp and Momentum, making it efficient and widely used.\n",
        "\n",
        "     Stochastic Gradient Descent (SGD): Uses random subsets of data for updates, improving speed for large datasets. Example of using Adam optimizer in Python:"
      ],
      "metadata": {
        "id": "SVGb7j4EU4Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "'''"
      ],
      "metadata": {
        "id": "RypT5iOeVDi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model?\n",
        " - sklearn.linear_model is a module in the scikit-learn library that provides tools for implementing linear models in Python, such as linear regression, logistic regression, and ridge regression. For example, LinearRegression() can be used to fit a linear model to a dataset:"
      ],
      "metadata": {
        "id": "neNBMpirVCzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "'''"
      ],
      "metadata": {
        "id": "YgVF0Ml4VKz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        " - The model.fit() method is used to train a Machine Learning model on the provided data by finding patterns and learning the optimal parameters. Arguments:\n",
        "\n",
        "     * X: Features (input data).\n",
        "\n",
        "     * y: Target (output labels).\n",
        "     \n",
        "     Example:"
      ],
      "metadata": {
        "id": "pStUFvhEVXCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model.fit(X_train, y_train)\n",
        "'''"
      ],
      "metadata": {
        "id": "LhngTUs1VoQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        " - The model.predict() method is used to make predictions on new, unseen data after the model has been trained.\n",
        "\n",
        " Arguments:\n",
        "\n",
        "      * X: Features of the new data.\n",
        "      \n",
        "    Example:"
      ],
      "metadata": {
        "id": "rX4FDMd1Vusi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "predictions = model.predict(X_test)\n",
        "'''"
      ],
      "metadata": {
        "id": "sgR65DDFWDJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        " - Continuous Variables: Take numeric values within a range (e.g., height, temperature).\n",
        "\n",
        "     Categorical Variables: Represent discrete categories or labels (e.g., gender: male/female, colors: red/blue)."
      ],
      "metadata": {
        "id": "_7j5262JWGC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        " - Feature scaling is a technique to standardize the range of independent variables so that they contribute equally to the model. Without scaling, models like SVM or KNN may give undue importance to features with larger ranges. Common techniques include normalization (scaling to [0,1]) and standardization (mean=0, standard deviation=1)."
      ],
      "metadata": {
        "id": "PeX76bhdWL7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. 22. How do we perform scaling in Python?\n",
        " - In Python, scaling can be performed using scikit-learn's StandardScaler or MinMaxScaler.\n",
        "\n",
        " Example: as shown in the code\n",
        "\n",
        " This scales the features to have a mean of 0 and standard deviation of 1."
      ],
      "metadata": {
        "id": "u39qXHedWc6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "'''"
      ],
      "metadata": {
        "id": "23vQXHtyWklJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing??\n",
        " - sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data. It includes functions for scaling (StandardScaler), normalizing (Normalizer), encoding categorical variables (OneHotEncoder), and more. Preprocessing ensures raw data is transformed into a format suitable for Machine Learning models."
      ],
      "metadata": {
        "id": "5QTnh7IyWvAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        " - Data splitting is done using the train_test_split function from scikit-learn.\n",
        "\n",
        " Example: as shown in the code\n",
        "\n",
        "\n",
        " Here, 80% of the data is used for training, and 20% is used for testing."
      ],
      "metadata": {
        "id": "3oRh1r5WW0QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "'''"
      ],
      "metadata": {
        "id": "UaWXL9JPW9P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding.\n",
        "\n",
        " - Data encoding is the process of converting categorical variables into numerical formats for Machine Learning. Common techniques include:\n",
        "\n",
        "     One-Hot Encoding: Converts categories into binary columns.\n",
        "\n",
        "     Label Encoding: Assigns integers to categories.\n",
        "     \n",
        "  For example:"
      ],
      "metadata": {
        "id": "zvMA-2rnXL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(categorical_data)\n",
        "'''"
      ],
      "metadata": {
        "id": "NoEg7H4CXgJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}